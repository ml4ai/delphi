{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backcasting Demo Notebook\n",
    "\n",
    "_Loren Champlin_\n",
    "\n",
    "Adapted from _Adarsh Pyarelal_'s WM 12 Month Evaluation Notebook \n",
    "\n",
    "As always, we begin with imports, and print out the commit hash for a rendered\n",
    "version of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "from delphi.visualization import visualize\n",
    "import delphi.jupyter_tools as jt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from delphi.db import engine\n",
    "jt.print_commit_hash_message()\n",
    "import random as rm\n",
    "import delphi.evaluation as EN\n",
    "import delphi.AnalysisGraph as AG\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(87)\n",
    "rm.seed(87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the Causal Analysis Graph (CAG). This is CAG was inferred by reading in a JSON corpus and was pruned and adjusted to be human migration centered. Also is a list of the nodes contained in the CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../scripts/build/migration_centered_CAG.pkl\",'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "for n in G.nodes:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we map indicator variables to nodes. For the most part indicator variables can be inferred from available data and texts, but we can also manually map indicators to nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.map_concepts_to_indicators()\n",
    "\n",
    "G.set_indicator(\"UN/events/human/human_migration\", \"New asylum seeking applicants\", \"UNHCR\")\n",
    "G.set_indicator(\"UN/entities/human/financial/economic/market\", \"Inflation Rate\", \"ieconomics.com\")\n",
    "G.set_indicator(\"UN/entities/human/food/food_security\", \"IPC Phase Classification\", \"FEWSNET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is also a list of the indicator variables in the same order as the list of nodes above (i.e \"Claims on other sectors of the domestic economy\" is attached to \"UN/events/human/economic_crisis\"\n",
    "                                                                                              \n",
    "                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in G.nodes(data=True):\n",
    "    for indicators in n[1][\"indicators\"].values():\n",
    "        print(indicators.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we visualize the CAG parameterized with indicator values for January, 2012. Also note that you can specifiy units for a particular indicator variables using a dictionary object where the keys are the indicator variable names and the values are the specified units. Default units are used if the selected units for an indicator variable do not exist. \n",
    "\n",
    "Legend for visualization: \n",
    "- Red edge: overall inhibition, green edge: overall promotion\n",
    "- Edge thickness corresponds roughly to the 'strength' of the influence.\n",
    "- Edge opacity corresponds roughly to the number of evidence fragments \n",
    "  that support the causal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = {\"Claims on other sectors of the domestic economy\": \"annual growth as % of broad money\"}\n",
    "G.parameterize(year=2012, units=units)\n",
    "visualize(G, indicators=True, indicator_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the inference model for the Casual Analysis Graph. Below you can see that the CAG G is passed to the train_model function. \n",
    "\n",
    "Other important arguments are:\n",
    "- start_year: The initial year to start training from.\n",
    "- start_month: The initial month to start training from.\n",
    "- end_year: The ending year for training.\n",
    "- end_month: The ending month for training. \n",
    "\n",
    "The above arguments ensures that the model is trained with the appropriate data given a time range.\n",
    "\n",
    "The second last argument shown is the the sample resolution (current seen set at 1000, default is 200). \n",
    "\n",
    "The last argument passed is a scale parameter for setting the \"standard deviation\" for a set of data values for each indicator variable given a time range. This affects the standard deviation of predictions. \n",
    "\n",
    "The train_model function can also take in all the parameter arguments as parameterize allowing for the setting of country, state, units, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN.train_model(G,2015,1,2015,12,1000,30000,k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function generates predictions for a given time range. Like train_model this takes a set of arguments start_year, start_month, end_year, and end_month that specify the time range for predictions. \n",
    "\n",
    "*Note: The predictions can be heavily reliant of the initial conditions, which are determined by the initial date of the prediction range (i.e, I suspect there is an initial condition bias). It still remains to be tested whether or not starting predictions from the initial training date or starting at the end of the training range yields more accuracte predictions. For example if training from January, 2015 to December, 2015 and we want to get predictions for January, 2016 to December, 2016, is it better to start predicting from January, 2015 or at the start of the dates we want (January, 2016). Initiating predictions at one time step before the prediction range is also a possibility for the most accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN.generate_predictions(G,2016,1,2016,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the predictions have been generated, there are several options for output. First is the to just return the raw predictions for a given indicator variable in a numpy array. This allows one to do there own plotting and manipulations. \n",
    "\n",
    "*Note: True data values from the delphi database can be retrieved using the data_to_df function in evaluation.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN.pred_to_array(G,'New asylum seeking applicants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation.py module can also output a pandas dataframe with the mean of the predictions along with a specified confidence interval for a given indicator variable. There are also options for presenting the true values, residuals, and error bounds based off of the residuals. \n",
    "\n",
    "*Note: Setting true_vals = True assumes that real data values exist in the database that match the time points of the predictions. Since the data retrieval function is set to return heuristic estimates for missing data values, then it's possible to have completely \"made-up\" true data if none actually exist for the prediction time range. Also whatever the mean_pred_to_df function should be passed the same country, state, units arguments as train_model (if any were passed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN.mean_pred_to_df(G,'New asylum seeking applicants',true_vals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get a plots representing the same data shown above. \n",
    "\n",
    "The plot types are:\n",
    "- Prediction: Shows only the predictions with specified confidence intervals. This is the default setting.\n",
    "- Comparison: Shows the predictions and confidence intervals along with a curve representing the true data values.\n",
    "- Error: Plots just the error with the error bounds along with a red reference line at 0. \n",
    "\n",
    "*Note: The above note for mean_pred_to_df also holds true for the Comparison and Error plot type. Also any other string argument passed to plot_type results in the defaults in the 'Prediction' plot type. The save_as argument can be set to a filename (with extension) to save the plot as a file (e.g, save_as = pred_plot.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN.pred_plot(G,'New asylum seeking applicants',plot_type='Comparison',save_as=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
